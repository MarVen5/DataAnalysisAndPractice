{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **数据分析及实践 Lab 2**\n",
    "\n",
    "### **PART 1**\n",
    "\n",
    "进入 `Nature` 主页，通过高级检索功能，搜索关键词 `llm`，限制年份为 `2023-2024`，搜索得到近两年间关键词含有 `llm` 的文章，使用 `python` 代码检索结果第一页的 `html` 内容并解码为文本串，将其以 `UTF-8` 编码格式写入 `page1.txt` 文件中，用于后续处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # 用于发送 HTTP 请求\n",
    "from bs4 import BeautifulSoup # 用于解析 HTML 内容\n",
    "\n",
    "url = 'https://www.nature.com/search?q=llm&order=relevance&date_range=2023-2024&page=1'\n",
    "\n",
    "response = requests.get(url) # 发送 GET 请求获取网页内容\n",
    "response.encoding = 'utf-8' # 设置响应的编码为 UTF-8\n",
    "\n",
    "with open ('page1.txt', 'w', encoding = 'utf-8') as f: # 打开文件 'page1.txt'，以写入模式保存页面内容\n",
    "    f.write(response.text)  # 将网页的 HTML 内容写入文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PART 2**\n",
    "\n",
    "打开 `page1.txt`，观察相关数据的组织格式规律。从这些文本串中提取一些论文相关的重要信息（包括文章标题，文章地址，文章简介，作者列表，文章类型，期刊名称，卷宗/页面信息），并按照发表的期刊名进行分类，存储到一个字典列表中。基于以上结果，输出每个期刊在 `page1` 中包含的论文数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nature Machine Intelligence                       The number of papers: 6\n",
      "Nature Communications                             The number of papers: 10\n",
      "Scientific Reports                                The number of papers: 12\n",
      "Nature Methods                                    The number of papers: 1\n",
      "npj Digital Medicine                              The number of papers: 3\n",
      "Nature Medicine                                   The number of papers: 2\n",
      "Nature                                            The number of papers: 4\n",
      "Nature Human Behaviour                            The number of papers: 2\n",
      "npj Precision Oncology                            The number of papers: 1\n",
      "Nature Computational Science                      The number of papers: 2\n",
      "BDJ Open                                          The number of papers: 1\n",
      "Nature Reviews Urology                            The number of papers: 1\n",
      "Communications Materials                          The number of papers: 1\n",
      "npj Biodiversity                                  The number of papers: 1\n",
      "Humanities and Social Sciences Communications     The number of papers: 1\n",
      "Eye                                               The number of papers: 1\n",
      "npj Computational Materials                       The number of papers: 1\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# 解析 HTML 内容\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 存储期刊论文信息的字典\n",
    "journal_paper_dict = {}\n",
    "\n",
    "# 遍历所有文章\n",
    "for article in soup.find_all('article'):\n",
    "    # 定义一个通用函数，用于获取指定标签的文本内容\n",
    "    def get_element(cls=None, tag=None, default=\"\"):\n",
    "        \"\"\"获取指定HTML标签的文本内容，若不存在则返回默认值\"\"\"\n",
    "        element = article.find(tag, class_=cls) if cls else article.find(tag)\n",
    "        return element.text.strip() if element else default\n",
    "\n",
    "    # 提取文章标题，如果不存在则返回 \"No Title\"\n",
    "    title = get_element(tag='h3', default=\"No Title\")\n",
    "\n",
    "    # 提取文章简介，如果不存在则返回 \"no description\"\n",
    "    description = get_element(tag='p', default=\"no description\")\n",
    "\n",
    "    # 提取文章链接，如果没有链接，则返回 \"#\"\n",
    "    url = (article.find('a')['href'] if article.find('a') else \"#\").strip()\n",
    "\n",
    "    # 提取文章作者信息\n",
    "    authors_ul = article.find('ul', class_=\"c-author-list c-author-list--compact c-author-list--truncated\")\n",
    "    authors_compact = [li.text.strip() for li in authors_ul.find_all('li')] if authors_ul else []\n",
    "\n",
    "    # 查找包含元数据信息的 div\n",
    "    meta_items = article.find('div', class_=\"c-card__section c-meta\") or {}\n",
    "\n",
    "    # 提取文章类型、期刊名称、卷宗/页面信息\n",
    "    meta_data = {\n",
    "        'type': meta_items.find('span', class_=\"c-meta__type\"),  # 文章类型\n",
    "        'journal': meta_items.find('div', class_=\"c-meta__item--block-at-lg\"),  # 期刊名称\n",
    "        'volume': meta_items.find('div', class_=\"c-meta__item--block-at-lg\")  # 卷宗/页面信息\n",
    "    }\n",
    "\n",
    "    # 组织每篇文章的信息\n",
    "    paper_info = {\n",
    "        \"title\": title,\n",
    "        \"authors\": authors_compact or [\"Anonymous\"],  # 若无作者信息，则标记为 \"Anonymous\"\n",
    "        \"url\": f\"{url}\",\n",
    "        \"description\": description,\n",
    "        \"type\": meta_data['type'].text.strip() if meta_data['type'] else \"Unknown\",\n",
    "        \"volume_page_info\": meta_data['volume'].text.strip() if meta_data['volume'] else \"\"\n",
    "    }\n",
    "\n",
    "    # 获取期刊名称，若无信息则标记为 \"Unknown Journal\"\n",
    "    journal = meta_data['journal'].text.strip() if meta_data['journal'] else \"Unknown Journal\"\n",
    "\n",
    "    # 使用 setdefault 避免重复判断 journal 是否在字典中\n",
    "    journal_paper_dict.setdefault(journal, {\"journal\": journal, \"papers\": []})[\"papers\"].append(paper_info)\n",
    "\n",
    "# 将字典转换为列表格式\n",
    "article_list = list(journal_paper_dict.values())\n",
    "\n",
    "# 将数据保存为 JSON 文件\n",
    "json.dump(article_list, open('nature_llm_before.json', 'w', encoding='utf-8'), indent=2, ensure_ascii=False)\n",
    "\n",
    "# 按格式输出每个期刊的论文数量\n",
    "for journal in article_list:\n",
    "    print(f'{journal[\"journal\"]:<50}The number of papers: {len(journal[\"papers\"])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PART 3**\n",
    "\n",
    "观察可以发现该文章的作者信息实际上多于搜索结果中展示的内容，请你仔细观察此界面的 `html` 数据组织格式，依据此编写 `python` 程序，将上一步骤中的字典提取内容中的作者列表中的内容进行替换，替换为文章主页面显示的全部作者。将上一步获得的字典列表转化为 `json` 对象，并以 2 字符缩进的方式写入 `nature llm.json` 文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_author_from_jsonld(json_data):\n",
    "    return [author[\"name\"] for author in json_data[\"mainEntity\"][\"author\"]]\n",
    "\n",
    "def fetch_article_details(url):\n",
    "    full_url = f'https://www.nature.com{url}' if not url.startswith('http') else url\n",
    "    response = requests.get(full_url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    response.encoding = 'utf-8'\n",
    "    return response.text\n",
    "\n",
    "def parse_jsonld(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    script = soup.find('script', type=\"application/ld+json\")\n",
    "    if script:\n",
    "        try:\n",
    "            return json.loads(script.string.strip())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "for journal_paper in article_list:\n",
    "    for paper in journal_paper[\"papers\"]:\n",
    "        html = fetch_article_details(paper[\"url\"])\n",
    "        if not html:\n",
    "            continue\n",
    "            \n",
    "        json_data = parse_jsonld(html)\n",
    "        if json_data:\n",
    "            paper[\"authors\"] = get_author_from_jsonld(json_data)\n",
    "        else:\n",
    "            paper[\"authors\"] = [\"Authors not found\"]\n",
    "\n",
    "json.dump(article_list, open('nature_llm.json', 'w', encoding='utf-8'), indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PART 4**\n",
    "\n",
    "将上一步获得的字典列表转化为 `json` 对象，并以 2 字符缩进的方式写入 `nature llm.json` 文件中。已经在 **PART 3** 代码中实现。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
